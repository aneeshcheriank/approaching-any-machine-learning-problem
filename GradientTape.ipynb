{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeshcheriank/approaching-any-machine-learning-problem/blob/main/GradientTape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UejkP1H-W7g8",
        "outputId": "30ee02a5-a58e-45b9-a427-906712969e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "11501568/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
        "import numpy as np\n",
        "\n",
        "# data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# give the color dim to the images\n",
        "x_train = np.expand_dims(x_train, 3)\n",
        "x_test = np.expand_dims(x_test, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PKYADKnlYKwR"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (2, 2), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPool2D((2, 2)),\n",
        "    Conv2D(64, (2, 2), activation='relu'),\n",
        "    MaxPool2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQQFgIEZdrme",
        "outputId": "50521f3d-5d24-446e-d5df-efee8cf5c398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2/100 loss = 11.3049\n",
            "epoch: 4/100 loss = 8.0784\n",
            "epoch: 6/100 loss = 5.0259\n",
            "epoch: 8/100 loss = 3.7428\n",
            "epoch: 10/100 loss = 2.4021\n",
            "epoch: 12/100 loss = 1.9598\n",
            "epoch: 14/100 loss = 1.9526\n",
            "epoch: 16/100 loss = 1.5451\n",
            "epoch: 18/100 loss = 1.1819\n",
            "epoch: 20/100 loss = 0.9817\n",
            "epoch: 22/100 loss = 0.8455\n",
            "epoch: 24/100 loss = 0.7566\n",
            "epoch: 26/100 loss = 0.7357\n",
            "epoch: 28/100 loss = 0.7027\n",
            "epoch: 30/100 loss = 0.6352\n",
            "epoch: 32/100 loss = 0.5784\n",
            "epoch: 34/100 loss = 0.5439\n",
            "epoch: 36/100 loss = 0.5102\n",
            "epoch: 38/100 loss = 0.4722\n",
            "epoch: 40/100 loss = 0.4408\n",
            "epoch: 42/100 loss = 0.4188\n",
            "epoch: 44/100 loss = 0.3979\n",
            "epoch: 46/100 loss = 0.3739\n",
            "epoch: 48/100 loss = 0.3509\n",
            "epoch: 50/100 loss = 0.3303\n",
            "epoch: 52/100 loss = 0.3122\n",
            "epoch: 54/100 loss = 0.2956\n",
            "epoch: 56/100 loss = 0.2792\n",
            "epoch: 58/100 loss = 0.2643\n",
            "epoch: 60/100 loss = 0.2503\n",
            "epoch: 62/100 loss = 0.2369\n",
            "epoch: 64/100 loss = 0.2251\n",
            "epoch: 66/100 loss = 0.2133\n",
            "epoch: 68/100 loss = 0.2015\n",
            "epoch: 70/100 loss = 0.1911\n",
            "epoch: 72/100 loss = 0.1815\n",
            "epoch: 74/100 loss = 0.1724\n",
            "epoch: 76/100 loss = 0.1641\n",
            "epoch: 78/100 loss = 0.1565\n",
            "epoch: 80/100 loss = 0.1492\n",
            "epoch: 82/100 loss = 0.1425\n",
            "epoch: 84/100 loss = 0.1361\n",
            "epoch: 86/100 loss = 0.1302\n",
            "epoch: 88/100 loss = 0.1247\n",
            "epoch: 90/100 loss = 0.1195\n",
            "epoch: 92/100 loss = 0.1147\n",
            "epoch: 94/100 loss = 0.1102\n",
            "epoch: 96/100 loss = 0.1059\n",
            "epoch: 98/100 loss = 0.1019\n",
            "epoch: 100/100 loss = 0.0981\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "  # gradient tape\n",
        "  with tf.GradientTape() as t:\n",
        "    pred = model(x_train)\n",
        "    l = loss(\n",
        "      y_train, pred # pytorch the order is (pred, actual)\n",
        "    )\n",
        "\n",
        "  # calculate the gradients\n",
        "  grads = t.gradient(\n",
        "    l, model.trainable_variables\n",
        "  )\n",
        "  # apply the gradients on the model variables\n",
        "  optimizer.apply_gradients(\n",
        "    zip(\n",
        "        grads, \n",
        "        model.trainable_variables\n",
        "       )\n",
        "  )\n",
        "  if (epoch+1)%2 == 0:\n",
        "    print(f'epoch: {epoch+1}/{EPOCHS} loss = {l.numpy():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "def accuracy(model, x_test, y_test):\n",
        "  '''\n",
        "  predict the accuracy of the model\n",
        "  input:\n",
        "  model, x_test, y_test\n",
        "  output: accuracy in percentage\n",
        "  '''\n",
        "  pred = model(x_test)\n",
        "  pred = tf.math.argmax(pred, axis=1)\n",
        "  correct = (pred == y_test).numpy().sum()\n",
        "  n_samples = x_test.shape[0]\n",
        "  acc = correct/n_samples * 100\n",
        "\n",
        "  return acc\n",
        "\n",
        "accuracy(model, x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxj9hFfp17Xt",
        "outputId": "81426cc8-43f8-4064-fd6a-949893373236"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.5"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(x_test)\n",
        "pred = tf.math.argmax(pred, axis=1)\n",
        "correct = (pred == y_test).numpy().sum()\n",
        "n_samples = x_test.shape[0]\n",
        "\n",
        "acc = correct/n_samples * 100\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJEYzpkY4Ggl",
        "outputId": "e31c22a1-b146-4a17-f0e5-0f6d322233f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.5"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader in Tensorflow"
      ],
      "metadata": {
        "id": "36NbSHh2ysUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9m5xAN4GePfG"
      },
      "outputs": [],
      "source": [
        "BUFFER = 1024\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\\\n",
        ".shuffle(buffer_size=BUFFER)\\\n",
        ".batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to test the data loader\n",
        "iterator = iter(train_data)\n",
        "data, label = iterator.next()\n",
        "\n",
        "print(f'data shape: {data.shape}')\n",
        "print(f'label shape: {label.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4bxdiCmh81r",
        "outputId": "23bf7540-da13-4ca9-9463-333c629b0ec8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data shape: (128, 28, 28, 1)\n",
            "label shape: (128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Training"
      ],
      "metadata": {
        "id": "ScHJH3dc8E1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "  # batch training\n",
        "  total_loss = 0\n",
        "  for x_batch, y_batch in train_data:\n",
        "    # gradient tape\n",
        "    with tf.GradientTape() as t:\n",
        "      pred = model(x_batch)\n",
        "      l = loss(\n",
        "        y_batch, pred # pytorch the order is (pred, actual)\n",
        "      )\n",
        "\n",
        "    # calculate the gradients\n",
        "    grads = t.gradient(\n",
        "      l, model.trainable_variables\n",
        "    )\n",
        "    # apply the gradients on the model variables\n",
        "    optimizer.apply_gradients(\n",
        "      zip(\n",
        "          grads, \n",
        "          model.trainable_variables\n",
        "         )\n",
        "    )\n",
        "    total_loss += l.numpy()\n",
        "  if (epoch+1)%2 == 0:\n",
        "    print(f'epoch: {epoch+1}/{EPOCHS} loss = {total_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gniaNNiWh97J",
        "outputId": "59606aa4-dcb1-46c3-e911-8214129cb0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2/100 loss = 34.3231\n",
            "epoch: 4/100 loss = 17.2616\n",
            "epoch: 6/100 loss = 11.5943\n",
            "epoch: 8/100 loss = 8.4373\n",
            "epoch: 10/100 loss = 7.3869\n",
            "epoch: 12/100 loss = 6.5979\n",
            "epoch: 14/100 loss = 6.0831\n",
            "epoch: 16/100 loss = 3.6101\n",
            "epoch: 18/100 loss = 5.2228\n",
            "epoch: 20/100 loss = 2.9344\n",
            "epoch: 22/100 loss = 3.3705\n",
            "epoch: 24/100 loss = 4.1665\n",
            "epoch: 26/100 loss = 4.1016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EwzT3Uwp7q48"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "GradientTape",
      "provenance": [],
      "authorship_tag": "ABX9TyO/5ou34YqPOikIPaTRYrP/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}